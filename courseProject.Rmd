---
title: Human Activity Recognition
author: "Banuçiçek Gürcüoglu"
output: html_document
job: Practical Machine Learning Course of John Hopkins University's Coursera Data Science Specialization
---
For building the model, we are going to use random forest algorithm from the caret package. To establish the quality of the data that is fed into the model, we will make sure that the null values like "NA", "","#DIV/0!" is handled (while loading the data) so that numeric columns are not treated as factors and remove the columns that contain mostly null values.

For validating the model, we will set 30% of the data aside and traing the model on the rest of it. Since we know the labels for the validation set, we will compare them to our predictions and get the out of sample error.


Load the libraries and human activity data:
```{r loaddata, eval=TRUE}

library(caret)
library(rattle)
dat = read.csv("pml-training.csv",na.strings = c("NA", "","#DIV/0!"))
testdat = read.csv("pml-testing.csv",na.strings = c("NA", "","#DIV/0!"))


```


Remove the columns that are irrelevant to tracking movement (first seven):
```{r irrelevantremoval, eval=TRUE}
names(dat)[1:7]
datRelevant <- dat[,-(1:7)]
testdatRelevant <- testdat[,-(1:7)]

```


Remove mostly NA columns:
```{r NAremoval, eval=TRUE}
noOfNAs <- sapply(datRelevant, function(x) sum(is.na(x)) )

nonNAcolumns <- noOfNAs == 0
datRelevantWithoutNA <- datRelevant[,nonNAcolumns]
testdatRelevantWithoutNA <- testdatRelevant[,nonNAcolumns]

```




Partition the data for 70% training and 30% validating:
```{r partition, eval=TRUE}
set.seed(42)
inTrain <- createDataPartition(y=datRelevantWithoutNA$classe,p=0.7, list=FALSE)
training <- datRelevantWithoutNA[inTrain,]
validating <- datRelevantWithoutNA[-inTrain,]
testing <- testdatRelevantWithoutNA



```

Training the model:

```{r trainModel, eval=TRUE}
mod1 <- train(classe ~.,method="rf",data=training)

```

Validating the model:

```{r validateModel, eval=TRUE}

validationPrediction <- predict(mod1,newdata=validating)
confusionMatrix(validating$classe, validationPrediction)

```
Accuracy on the validation test is 0.9932, therefor the out of the sample set error is 0.0068.

Prediction on test data:
```{r testPrediction, eval=TRUE}
testPrediction <- predict(mod1,newdata=testing)
testPrediction

```
